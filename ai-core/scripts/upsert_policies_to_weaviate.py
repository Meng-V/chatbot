#!/usr/bin/env python3
"""
Upsert Oxford Policy Data to Weaviate

Reads JSONL files generated by ingest_libguides_policies_oxford.py and
upserts them to Weaviate collections: CirculationPolicies and CirculationPolicyFacts.

Usage:
    python -m scripts.upsert_policies_to_weaviate
"""

import os
import sys
import json
import asyncio
from pathlib import Path
from typing import List, Dict, Any
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings

# Load environment
repo_root = Path(__file__).resolve().parent.parent.parent
load_dotenv(dotenv_path=repo_root / ".env")

sys.path.insert(0, str(repo_root / "ai-core"))
from utils.weaviate_client import get_weaviate_client


async def create_collections_if_needed(client):
    """Create Weaviate collections if they don't exist."""
    
    # Check CirculationPolicies collection
    if not client.collections.exists("CirculationPolicies"):
        print("üì¶ Creating CirculationPolicies collection...")
        client.collections.create(
            name="CirculationPolicies",
            description="Oxford circulation and borrowing policy content chunks",
            properties=[
                {"name": "canonical_url", "dataType": ["text"]},
                {"name": "source_url", "dataType": ["text"]},
                {"name": "title", "dataType": ["text"]},
                {"name": "section_path", "dataType": ["text"]},
                {"name": "campus_scope", "dataType": ["text"]},
                {"name": "topic", "dataType": ["text"]},
                {"name": "audience", "dataType": ["text"]},
                {"name": "keywords", "dataType": ["text[]"]},
                {"name": "chunk_text", "dataType": ["text"]},
            ]
        )
        print("‚úÖ CirculationPolicies collection created")
    else:
        print("‚úÖ CirculationPolicies collection already exists")
    
    # Check CirculationPolicyFacts collection
    if not client.collections.exists("CirculationPolicyFacts"):
        print("üì¶ Creating CirculationPolicyFacts collection...")
        client.collections.create(
            name="CirculationPolicyFacts",
            description="Oxford policy fact cards for direct Q&A",
            properties=[
                {"name": "campus_scope", "dataType": ["text"]},
                {"name": "fact_type", "dataType": ["text"]},
                {"name": "question_patterns", "dataType": ["text[]"]},
                {"name": "answer", "dataType": ["text"]},
                {"name": "canonical_url", "dataType": ["text"]},
                {"name": "source_url", "dataType": ["text"]},
                {"name": "anchor_hint", "dataType": ["text"]},
                {"name": "tags", "dataType": ["text[]"]},
            ]
        )
        print("‚úÖ CirculationPolicyFacts collection created")
    else:
        print("‚úÖ CirculationPolicyFacts collection already exists")


async def load_jsonl(filepath: Path) -> List[Dict[str, Any]]:
    """Load JSONL file."""
    items = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                items.append(json.loads(line))
    return items


async def upsert_chunks(client, chunks: List[Dict[str, Any]], embeddings: OpenAIEmbeddings):
    """Upsert policy chunks to CirculationPolicies collection."""
    print(f"\nüìÑ Upserting {len(chunks)} chunks to CirculationPolicies...")
    
    collection = client.collections.get("CirculationPolicies")
    
    batch_size = 50
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i+batch_size]
        
        # Generate embeddings for batch
        texts = [chunk["chunk_text"] for chunk in batch]
        vectors = await embeddings.aembed_documents(texts)
        
        # Upsert with deterministic IDs
        with collection.batch.dynamic() as batch_obj:
            for chunk, vector in zip(batch, vectors):
                chunk_id = chunk["id"]
                properties = {k: v for k, v in chunk.items() if k != "id"}
                batch_obj.add_object(
                    properties=properties,
                    uuid=chunk_id,
                    vector=vector
                )
        
        print(f"   ‚úÖ Processed {min(i+batch_size, len(chunks))}/{len(chunks)} chunks")
    
    print("‚úÖ All chunks upserted")


async def upsert_facts(client, facts: List[Dict[str, Any]], embeddings: OpenAIEmbeddings):
    """Upsert policy facts to CirculationPolicyFacts collection."""
    print(f"\nüí° Upserting {len(facts)} facts to CirculationPolicyFacts...")
    
    collection = client.collections.get("CirculationPolicyFacts")
    
    batch_size = 50
    for i in range(0, len(facts), batch_size):
        batch = facts[i:i+batch_size]
        
        # Generate embeddings for batch (use first question pattern + answer)
        texts = []
        for fact in batch:
            question = fact["question_patterns"][0] if fact["question_patterns"] else ""
            answer = fact["answer"]
            combined = f"{question} {answer}"
            texts.append(combined)
        
        vectors = await embeddings.aembed_documents(texts)
        
        # Upsert with deterministic IDs
        with collection.batch.dynamic() as batch_obj:
            for fact, vector in zip(batch, vectors):
                fact_id = fact["id"]
                properties = {k: v for k, v in fact.items() if k != "id"}
                batch_obj.add_object(
                    properties=properties,
                    uuid=fact_id,
                    vector=vector
                )
        
        print(f"   ‚úÖ Processed {min(i+batch_size, len(facts))}/{len(facts)} facts")
    
    print("‚úÖ All facts upserted")


async def main():
    """Main upsert pipeline."""
    print("=" * 80)
    print("Oxford Policy Data Upsert to Weaviate")
    print("=" * 80)
    
    # Check for required files
    data_dir = repo_root / "ai-core" / "data" / "policies"
    chunks_file = data_dir / "circulation_policies_oxford_chunks.jsonl"
    facts_file = data_dir / "circulation_policies_oxford_facts.jsonl"
    
    if not chunks_file.exists():
        print(f"‚ùå Chunks file not found: {chunks_file}")
        print("   Run: python -m scripts.ingest_libguides_policies_oxford")
        return
    
    if not facts_file.exists():
        print(f"‚ùå Facts file not found: {facts_file}")
        print("   Run: python -m scripts.ingest_libguides_policies_oxford")
        return
    
    # Load data
    print("\nüìÇ Loading JSONL files...")
    chunks = await load_jsonl(chunks_file)
    facts = await load_jsonl(facts_file)
    print(f"   ‚úÖ Loaded {len(chunks)} chunks, {len(facts)} facts")
    
    # Initialize embeddings
    print("\nüîë Initializing OpenAI embeddings...")
    openai_api_key = os.getenv("OPENAI_API_KEY", "")
    if not openai_api_key:
        print("‚ùå OPENAI_API_KEY not set")
        return
    
    embeddings = OpenAIEmbeddings(
        model="text-embedding-3-large",
        api_key=openai_api_key
    )
    print("   ‚úÖ Embeddings ready")
    
    # Connect to Weaviate
    print("\nüîå Connecting to Weaviate...")
    client = None
    try:
        client = get_weaviate_client()
        print("   ‚úÖ Connected to Weaviate")
        
        # Create collections
        await create_collections_if_needed(client)
        
        # Upsert data
        await upsert_chunks(client, chunks, embeddings)
        await upsert_facts(client, facts, embeddings)
        
        print("\n" + "=" * 80)
        print("üéâ Upsert complete!")
        print("=" * 80)
        print(f"\nCollections updated:")
        print(f"  ‚Ä¢ CirculationPolicies: {len(chunks)} chunks")
        print(f"  ‚Ä¢ CirculationPolicyFacts: {len(facts)} facts")
        
    except Exception as e:
        print(f"\n‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
    
    finally:
        if client:
            client.close()


if __name__ == "__main__":
    asyncio.run(main())
