#!/usr/bin/env python3
"""
å»é‡å’Œåˆå¹¶ç›¸ä¼¼çš„Q&Aå¯¹
ç”¨æ³•: python deduplicate_transcripts.py <input.json> [--output deduplicated.json]
"""
import json
import argparse
import numpy as np
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


def calculate_text_similarity(texts: List[str]) -> np.ndarray:
    """
    ä½¿ç”¨TF-IDFè®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ
    """
    if len(texts) < 2:
        return np.array([[1.0]])
    
    try:
        vectorizer = TfidfVectorizer(
            max_features=1000,
            stop_words='english',
            lowercase=True
        )
        tfidf_matrix = vectorizer.fit_transform(texts)
        similarity_matrix = cosine_similarity(tfidf_matrix)
        return similarity_matrix
    except Exception as e:
        print(f"âš ï¸  TF-IDF calculation error: {e}")
        # è¿”å›å•ä½çŸ©é˜µï¼ˆæ¯ä¸ªé—®é¢˜åªä¸è‡ªå·±ç›¸ä¼¼ï¼‰
        return np.eye(len(texts))


def deduplicate_qa_pairs(
    qa_pairs: List[Dict[str, Any]], 
    similarity_threshold: float = 0.85,
    merge_strategy: str = 'best'
) -> List[Dict[str, Any]]:
    """
    å»é‡Q&Aå¯¹
    
    Args:
        qa_pairs: Q&Aå¯¹åˆ—è¡¨
        similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0)
        merge_strategy: 'best' = ä¿ç•™æœ€é«˜è´¨é‡, 'merge' = åˆå¹¶ç­”æ¡ˆ
    
    Returns:
        å»é‡åçš„Q&Aå¯¹åˆ—è¡¨
    """
    if not qa_pairs:
        return []
    
    print(f"\nğŸ” å¼€å§‹å»é‡...")
    print(f"   åŸå§‹æ•°é‡: {len(qa_pairs)}")
    print(f"   ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
    print(f"   åˆå¹¶ç­–ç•¥: {merge_strategy}")
    
    # æå–æ‰€æœ‰é—®é¢˜
    questions = [qa['question'] for qa in qa_pairs]
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    print("   è®¡ç®—TF-IDFç›¸ä¼¼åº¦...")
    similarity_matrix = calculate_text_similarity(questions)
    
    # æ ‡è®°é‡å¤é¡¹å’Œåˆ†ç»„
    duplicates = set()
    duplicate_groups = []  # æ¯ä¸ªgroupæ˜¯ç›¸ä¼¼é—®é¢˜çš„ç´¢å¼•åˆ—è¡¨
    
    for i in range(len(qa_pairs)):
        if i in duplicates:
            continue
        
        # æ‰¾åˆ°æ‰€æœ‰ä¸é—®é¢˜iç›¸ä¼¼çš„é—®é¢˜
        similar_indices = []
        for j in range(i + 1, len(qa_pairs)):
            if j not in duplicates and similarity_matrix[i][j] >= similarity_threshold:
                similar_indices.append(j)
                duplicates.add(j)
        
        if similar_indices:
            # æœ‰ç›¸ä¼¼é—®é¢˜ï¼Œåˆ›å»ºä¸€ä¸ªgroup
            group = [i] + similar_indices
            duplicate_groups.append(group)
        else:
            # æ²¡æœ‰ç›¸ä¼¼é—®é¢˜ï¼Œå•ç‹¬æˆç»„
            duplicate_groups.append([i])
    
    print(f"   å‘ç° {len(duplicates)} ä¸ªé‡å¤é¡¹")
    print(f"   åˆå¹¶ä¸º {len(duplicate_groups)} ç»„")
    
    # æ ¹æ®ç­–ç•¥åˆå¹¶æ¯ä¸ªç»„
    deduplicated_pairs = []
    
    for group in duplicate_groups:
        if len(group) == 1:
            # å•ä¸€é—®é¢˜ï¼Œç›´æ¥ä¿ç•™
            deduplicated_pairs.append(qa_pairs[group[0]])
        else:
            # å¤šä¸ªç›¸ä¼¼é—®é¢˜ï¼Œæ ¹æ®ç­–ç•¥å¤„ç†
            group_pairs = [qa_pairs[idx] for idx in group]
            
            if merge_strategy == 'best':
                # ä¿ç•™è´¨é‡æœ€é«˜çš„
                best_qa = max(group_pairs, key=lambda x: (
                    x['confidence_score'],
                    x['rating'],
                    len(x['answer'])
                ))
                deduplicated_pairs.append(best_qa)
            
            elif merge_strategy == 'merge':
                # åˆå¹¶ç­”æ¡ˆï¼ˆä½¿ç”¨æœ€é«˜è´¨é‡çš„é—®é¢˜ï¼Œåˆå¹¶æ‰€æœ‰ä¸é‡å¤çš„ç­”æ¡ˆï¼‰
                best_qa = max(group_pairs, key=lambda x: (
                    x['confidence_score'],
                    x['rating']
                ))
                
                # æ”¶é›†æ‰€æœ‰ä¸åŒçš„ç­”æ¡ˆ
                unique_answers = []
                seen_answers = set()
                
                for qa in sorted(group_pairs, key=lambda x: x['confidence_score'], reverse=True):
                    answer_normalized = qa['answer'].lower().strip()
                    if answer_normalized not in seen_answers:
                        unique_answers.append(qa['answer'])
                        seen_answers.add(answer_normalized)
                
                # å¦‚æœæœ‰å¤šä¸ªä¸åŒç­”æ¡ˆï¼Œåˆå¹¶å®ƒä»¬
                if len(unique_answers) > 1:
                    merged_answer = '\n\n'.join([f"[Option {i+1}] {ans}" 
                                                  for i, ans in enumerate(unique_answers[:3])])
                    best_qa['answer'] = merged_answer
                
                # åˆå¹¶å…³é”®è¯å’Œæ ‡ç­¾
                all_keywords = set()
                all_tags = set()
                for qa in group_pairs:
                    all_keywords.update(qa.get('keywords', []))
                    all_tags.update(qa.get('tags', []))
                
                best_qa['keywords'] = list(all_keywords)[:10]  # æœ€å¤š10ä¸ªå…³é”®è¯
                best_qa['tags'] = list(all_tags)
                
                deduplicated_pairs.append(best_qa)
    
    print(f"âœ… å»é‡å®Œæˆ: {len(qa_pairs)} -> {len(deduplicated_pairs)}")
    print(f"   å‡å°‘: {len(qa_pairs) - len(deduplicated_pairs)} æ¡ ({((len(qa_pairs) - len(deduplicated_pairs)) / len(qa_pairs) * 100):.1f}%)")
    
    return deduplicated_pairs


def analyze_duplicates(qa_pairs: List[Dict[str, Any]], similarity_threshold: float = 0.85):
    """
    åˆ†æé‡å¤æƒ…å†µä½†ä¸ä¿®æ”¹æ•°æ®
    """
    if not qa_pairs:
        return
    
    print(f"\nğŸ“Š åˆ†æé‡å¤æƒ…å†µ...")
    
    questions = [qa['question'] for qa in qa_pairs]
    similarity_matrix = calculate_text_similarity(questions)
    
    # ç»Ÿè®¡ç›¸ä¼¼å¯¹æ•°é‡
    duplicate_count = 0
    high_similarity_pairs = []
    
    for i in range(len(qa_pairs)):
        for j in range(i + 1, len(qa_pairs)):
            if similarity_matrix[i][j] >= similarity_threshold:
                duplicate_count += 1
                if len(high_similarity_pairs) < 5:  # åªä¿ç•™å‰5ä¸ªä¾‹å­
                    high_similarity_pairs.append({
                        'q1': questions[i][:100] + '...' if len(questions[i]) > 100 else questions[i],
                        'q2': questions[j][:100] + '...' if len(questions[j]) > 100 else questions[j],
                        'similarity': similarity_matrix[i][j]
                    })
    
    print(f"\nå‘ç° {duplicate_count} å¯¹ç›¸ä¼¼é—®é¢˜ (é˜ˆå€¼={similarity_threshold})")
    
    if high_similarity_pairs:
        print("\nç¤ºä¾‹ç›¸ä¼¼é—®é¢˜å¯¹:")
        for idx, pair in enumerate(high_similarity_pairs, 1):
            print(f"\n  [{idx}] ç›¸ä¼¼åº¦: {pair['similarity']:.3f}")
            print(f"      Q1: {pair['q1']}")
            print(f"      Q2: {pair['q2']}")
    
    # æŒ‰ä¸»é¢˜ç»Ÿè®¡
    topic_stats = defaultdict(int)
    for qa in qa_pairs:
        topic_stats[qa.get('topic', 'unknown')] += 1
    
    print("\nä¸»é¢˜åˆ†å¸ƒ:")
    for topic, count in sorted(topic_stats.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {topic}: {count}")


def filter_by_quality(qa_pairs: List[Dict[str, Any]], min_confidence: float = 0.5) -> List[Dict[str, Any]]:
    """
    æŒ‰è´¨é‡è¿‡æ»¤
    """
    filtered = [qa for qa in qa_pairs if qa.get('confidence_score', 0) >= min_confidence]
    print(f"ğŸ” è´¨é‡è¿‡æ»¤: {len(qa_pairs)} -> {len(filtered)} (removed {len(qa_pairs) - len(filtered)})")
    return filtered


def main():
    parser = argparse.ArgumentParser(description='å»é‡å’Œåˆå¹¶ç›¸ä¼¼Q&Aå¯¹')
    parser.add_argument('input', help='è¾“å…¥JSONæ–‡ä»¶ (cleaned_transcripts.json)')
    parser.add_argument('--output', '-o', help='è¾“å‡ºJSONæ–‡ä»¶ (é»˜è®¤: deduplicated_transcripts.json)')
    parser.add_argument('--threshold', '-t', type=float, default=0.85,
                        help='ç›¸ä¼¼åº¦é˜ˆå€¼ (0.0-1.0, é»˜è®¤: 0.85)')
    parser.add_argument('--strategy', '-s', choices=['best', 'merge'], default='best',
                        help='åˆå¹¶ç­–ç•¥: best=ä¿ç•™æœ€é«˜è´¨é‡, merge=åˆå¹¶ç­”æ¡ˆ (é»˜è®¤: best)')
    parser.add_argument('--min-confidence', type=float, default=0.0,
                        help='æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)')
    parser.add_argument('--analyze-only', action='store_true',
                        help='åªåˆ†æä¸å»é‡')
    
    args = parser.parse_args()
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if not args.output:
        input_path = Path(args.input)
        args.output = str(input_path.parent / 'deduplicated_transcripts.json')
    
    print("ğŸš€ å¼€å§‹å¤„ç†...")
    print(f"è¾“å…¥æ–‡ä»¶: {args.input}")
    
    # åŠ è½½æ•°æ®
    try:
        with open(args.input, 'r', encoding='utf-8') as f:
            qa_pairs = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–è¾“å…¥æ–‡ä»¶: {e}")
        return
    
    print(f"âœ… åŠ è½½äº† {len(qa_pairs)} æ¡Q&Aå¯¹")
    
    # åªåˆ†ææ¨¡å¼
    if args.analyze_only:
        analyze_duplicates(qa_pairs, args.threshold)
        return
    
    # è´¨é‡è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
    if args.min_confidence > 0:
        qa_pairs = filter_by_quality(qa_pairs, args.min_confidence)
    
    # å»é‡
    deduplicated_pairs = deduplicate_qa_pairs(
        qa_pairs, 
        similarity_threshold=args.threshold,
        merge_strategy=args.strategy
    )
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(deduplicated_pairs, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… å»é‡æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“¦ æœ€ç»ˆæ•°é‡: {len(deduplicated_pairs)} æ¡Q&Aå¯¹")
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆç»Ÿè®¡")
    print("="*60)
    
    # è´¨é‡åˆ†å¸ƒ
    quality_ranges = {
        'Very High (â‰¥0.8)': sum(1 for qa in deduplicated_pairs if qa['confidence_score'] >= 0.8),
        'High (0.7-0.8)': sum(1 for qa in deduplicated_pairs if 0.7 <= qa['confidence_score'] < 0.8),
        'Medium (0.6-0.7)': sum(1 for qa in deduplicated_pairs if 0.6 <= qa['confidence_score'] < 0.7),
        'Low (<0.6)': sum(1 for qa in deduplicated_pairs if qa['confidence_score'] < 0.6)
    }
    
    print("\nè´¨é‡åˆ†å¸ƒ:")
    for range_name, count in quality_ranges.items():
        percentage = (count / len(deduplicated_pairs) * 100) if deduplicated_pairs else 0
        print(f"  - {range_name}: {count} ({percentage:.1f}%)")
    
    # ä¸»é¢˜åˆ†å¸ƒ
    topic_counts = defaultdict(int)
    for qa in deduplicated_pairs:
        topic_counts[qa['topic']] += 1
    
    print("\nä¸»é¢˜åˆ†å¸ƒ:")
    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(deduplicated_pairs) * 100) if deduplicated_pairs else 0
        print(f"  - {topic}: {count} ({percentage:.1f}%)")
    
    print("\n" + "="*60)
    print("âœ¨ ä¸‹ä¸€æ­¥: python scripts/ingest_transcripts.py")
    print("="*60)


if __name__ == '__main__':
    main()
