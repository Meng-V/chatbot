#!/usr/bin/env python3
"""
æ¸…ç†å’Œå¤„ç†å†å²å¯¹è¯CSVæ–‡ä»¶ï¼Œå‡†å¤‡RAGæ‘„å…¥
ç”¨æ³•: python clean_transcripts.py <csv_file1> [csv_file2] ... [--output output.json]
"""
import csv
import json
import re
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional
from collections import defaultdict

# ä¸»é¢˜å…³é”®è¯å­—å…¸ï¼ˆåŸºäºscope_definition.pyï¼‰
TOPIC_KEYWORDS = {
    'discovery_search': [
        'book', 'article', 'journal', 'database', 'catalog', 'primo', 
        'find', 'search', 'call number', 'isbn', 'doi', 'citation',
        'ebook', 'e-book', 'electronic', 'full text', 'pdf'
    ],
    'booking_or_hours': [
        'hours', 'open', 'close', 'room', 'reservation', 'book a room', 
        'study room', 'schedule', 'available', 'reserve'
    ],
    'policy_or_service': [
        'renew', 'return', 'fine', 'overdue', 'print', 'scan', 'copy',
        'borrow', 'interlibrary loan', 'ill', 'checkout', 'due date'
    ],
    'subject_librarian': [
        'librarian', 'subject specialist', 'research help', 'consultation',
        'libguide', 'research guide', 'subject guide', 'contact', 'who can help'
    ],
    'course_subject_help': [
        'course', 'class', 'assignment', 'professor', 'instructor',
        'eng ', 'psy ', 'chm ', 'bio ', 'guide for'
    ]
}

# è¶…å‡ºèŒƒå›´çš„å…³é”®è¯ï¼ˆéœ€è¦è¿‡æ»¤ï¼‰
OUT_OF_SCOPE_KEYWORDS = [
    'admission', 'tuition', 'housing', 'dining hall', 'parking',
    'canvas', 'blackboard', 'email account', 'password reset',
    'homework', 'test answer', 'solve this problem',
    'armstrong', 'rec center', 'student center'
]


def anonymize_librarian_name(name: str) -> str:
    """
    éšç§ä¿æŠ¤ï¼šå°†å›¾ä¹¦é¦†å‘˜å§“åæ›¿æ¢ä¸º"Librarian"
    ä¿ç•™"Patron"ä¸å˜
    """
    if not name or name.strip() == '':
        return ''
    
    name_stripped = name.strip()
    
    # ä¿ç•™Patronä¸å˜
    if name_stripped.lower() == 'patron':
        return 'Patron'
    
    # å…¶ä»–æ‰€æœ‰åå­—éƒ½æ›¿æ¢ä¸ºLibrarian
    return 'Librarian'


def parse_transcript(transcript_text: str, anonymize: bool = True) -> List[Dict[str, str]]:
    """
    è§£æTranscriptå­—æ®µï¼Œæå–ç»“æ„åŒ–æ¶ˆæ¯åˆ—è¡¨
    
    æ ¼å¼: "HH:MM:SS - Speaker Name : Message content"
    
    Args:
        transcript_text: å¯¹è¯æ–‡æœ¬
        anonymize: æ˜¯å¦åŒ¿ååŒ–å›¾ä¹¦é¦†å‘˜å§“åï¼ˆé»˜è®¤Trueï¼‰
    """
    messages = []
    if not transcript_text or transcript_text.strip() == '':
        return messages
    
    lines = transcript_text.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        # åŒ¹é…æ—¶é—´æˆ³ - è¯´è¯äºº : å†…å®¹
        match = re.match(r'(\d{2}:\d{2}(?::\d{2})?) - ([^:]+) : (.+)', line)
        if match:
            time, speaker, content = match.groups()
            
            # éšç§ä¿æŠ¤ï¼šæ›¿æ¢å›¾ä¹¦é¦†å‘˜å§“å
            if anonymize:
                speaker = anonymize_librarian_name(speaker)
            
            messages.append({
                'time': time.strip(),
                'speaker': speaker.strip(),
                'content': content.strip()
            })
    
    return messages


def clean_message_content(text: str) -> Optional[str]:
    """
    æ¸…ç†æ¶ˆæ¯å†…å®¹ï¼Œç§»é™¤HTMLæ ‡ç­¾ã€é“¾æ¥ç­‰å™ªéŸ³
    """
    if not text:
        return None
    
    # ç§»é™¤HTMLæ ‡ç­¾ä½†ä¿ç•™é“¾æ¥æ–‡æœ¬
    text = re.sub(r'<a href="[^"]*"[^>]*>([^<]*)</a>', r'\1', text)
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤"attached a file"ç±»å‹çš„æ¶ˆæ¯
    if 'attached a file:' in text.lower():
        return None
    
    # ç§»é™¤çº¯URLè¡Œ
    if text.startswith('http') and len(text.split()) == 1:
        return None
    
    # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦
    text = ' '.join(text.split())
    
    # è¿‡æ»¤è¿‡çŸ­çš„å¯’æš„è¯­ï¼ˆå°äº10ä¸ªå­—ç¬¦ä¸”åªåŒ…å«å¸¸è§å¯’æš„ï¼‰
    if len(text) < 10:
        greetings = ['hi', 'hello', 'thanks', 'thank you', "you're welcome", 
                     'ok', 'okay', 'sure', 'yes', 'no', 'got it']
        if text.lower().strip('!.?') in greetings:
            return None
    
    return text


def is_out_of_scope(text: str) -> bool:
    """
    æ£€æŸ¥æ–‡æœ¬æ˜¯å¦è¶…å‡ºå›¾ä¹¦é¦†æœåŠ¡èŒƒå›´
    """
    text_lower = text.lower()
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¶…èŒƒå›´å…³é”®è¯
    for keyword in OUT_OF_SCOPE_KEYWORDS:
        if keyword in text_lower:
            return True
    
    return False


def classify_topic(question: str, answer: str) -> str:
    """
    æ ¹æ®å…³é”®è¯è‡ªåŠ¨åˆ†ç±»ä¸»é¢˜
    """
    combined_text = (question + ' ' + answer).lower()
    
    # æ£€æŸ¥æ˜¯å¦è¶…å‡ºèŒƒå›´
    if is_out_of_scope(combined_text):
        return 'out_of_scope'
    
    # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„å¾—åˆ†
    scores = {}
    for topic, keywords in TOPIC_KEYWORDS.items():
        score = sum(1 for kw in keywords if kw in combined_text)
        scores[topic] = score
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
    if max(scores.values()) > 0:
        return max(scores, key=scores.get)
    
    return 'general_question'


def extract_keywords(text: str, top_n: int = 5) -> List[str]:
    """
    ç®€å•çš„å…³é”®è¯æå–ï¼ˆåŸºäºè¯é¢‘ï¼‰
    """
    # ç§»é™¤å¸¸è§åœç”¨è¯
    stopwords = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
                 'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',
                 'i', 'you', 'we', 'they', 'this', 'that', 'these', 'those', 'can', 'do'}
    
    # åˆ†è¯å¹¶ç»Ÿè®¡
    words = re.findall(r'\b[a-z]+\b', text.lower())
    word_freq = defaultdict(int)
    
    for word in words:
        if len(word) > 3 and word not in stopwords:
            word_freq[word] += 1
    
    # è¿”å›top Né«˜é¢‘è¯
    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)
    return [word for word, freq in sorted_words[:top_n]]


def calculate_confidence_score(qa: Dict[str, Any], metadata: Dict[str, Any]) -> float:
    """
    è®¡ç®—QAå¯¹çš„è´¨é‡ç½®ä¿¡åº¦ (0.0 - 1.0)
    """
    score = 0.5  # åŸºç¡€åˆ†
    
    # 1. ç”¨æˆ·è¯„åˆ†åŠ æƒ (æœ€é«˜+0.3)
    rating = metadata.get('rating', 0)
    if rating >= 4:
        score += 0.3
    elif rating >= 3:
        score += 0.2
    elif rating >= 2:
        score += 0.1
    
    # 2. ç­”æ¡ˆé•¿åº¦åŠ æƒ (æœ€é«˜+0.1)
    answer_len = len(qa.get('answer', ''))
    if 50 <= answer_len <= 500:
        score += 0.1
    elif 20 <= answer_len < 50 or 500 < answer_len <= 1000:
        score += 0.05
    
    # 3. ç­”æ¡ˆåŒ…å«URLé€šå¸¸è´¨é‡è¾ƒé«˜ (+0.1)
    answer = qa.get('answer', '')
    if 'http' in answer or 'lib.miamioh.edu' in answer:
        score += 0.1
    
    # 4. å¯¹è¯æ—¶é•¿åˆç† (+0.05)
    duration = metadata.get('duration', 0)
    if 30 <= duration <= 600:  # 30ç§’åˆ°10åˆ†é’Ÿ
        score += 0.05
    
    return min(1.0, score)


def extract_first_qa(messages: List[Dict], initial_question: str) -> Optional[Dict[str, str]]:
    """
    ç­–ç•¥A: æå–Initial Question + å›¾ä¹¦é¦†å‘˜çš„ç¬¬ä¸€ä¸ªå®è´¨æ€§å›ç­”
    æœ€ç®€å•å¿«é€Ÿçš„æ–¹æ³•
    """
    librarian_answer = None
    
    for msg in messages:
        if msg['speaker'] != 'Patron':  # å›¾ä¹¦é¦†å‘˜å›å¤
            content = clean_message_content(msg['content'])
            if content and len(content) > 15:  # å®è´¨æ€§å›ç­”
                librarian_answer = content
                break
    
    if librarian_answer and initial_question:
        return {
            'question': initial_question,
            'answer': librarian_answer
        }
    
    return None


def extract_all_qa_pairs(messages: List[Dict], initial_question: str) -> List[Dict[str, str]]:
    """
    ç­–ç•¥B: æå–æ‰€æœ‰Q&Aå¯¹ï¼ˆæ¨èï¼‰
    å°†å¤šè½®å¯¹è¯æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„Q&A
    """
    qa_pairs = []
    current_question = initial_question
    current_answer_parts = []
    
    for msg in messages:
        content = clean_message_content(msg['content'])
        if not content:
            continue
        
        if msg['speaker'] == 'Patron':
            # ç”¨æˆ·é—®é¢˜
            # å¦‚æœå·²æœ‰ç­”æ¡ˆï¼Œä¿å­˜å½“å‰Q&A
            if current_answer_parts and current_question:
                qa_pairs.append({
                    'question': current_question,
                    'answer': ' '.join(current_answer_parts)
                })
                current_answer_parts = []
            
            # æ›´æ–°å½“å‰é—®é¢˜ï¼ˆè¿‡æ»¤æ‰å¤ªçŸ­çš„æ¶ˆæ¯ï¼‰
            if len(content) > 15:
                current_question = content
        else:
            # å›¾ä¹¦é¦†å‘˜å›å¤
            if len(content) > 15:  # åªä¿ç•™å®è´¨æ€§å›å¤
                current_answer_parts.append(content)
    
    # ä¿å­˜æœ€åä¸€å¯¹
    if current_answer_parts and current_question:
        qa_pairs.append({
            'question': current_question,
            'answer': ' '.join(current_answer_parts)
        })
    
    return qa_pairs


def should_include_chat(row: Dict[str, str]) -> tuple[bool, str]:
    """
    åˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ…å«è¿™æ¡å¯¹è¯
    è¿”å›: (æ˜¯å¦åŒ…å«, åŸå› )
    """
    # 1. è¿‡æ»¤æ¶ˆæ¯æ•°è¿‡å°‘
    message_count = int(row.get('Message Count', 0))
    if message_count < 2:
        return False, 'too_few_messages'
    
    # 2. è¿‡æ»¤è¶…é•¿å¯¹è¯ï¼ˆå¯èƒ½æ˜¯å¤æ‚æ¡ˆä¾‹ï¼‰
    if message_count > 30:
        return False, 'too_long'
    
    # 3. è¿‡æ»¤ä½è¯„åˆ†ï¼ˆRating == 1ï¼‰
    rating = row.get('Rating (0-4)', '').strip()
    if rating and rating != '' and int(float(rating)) == 1:
        return False, 'low_rating'
    
    # 4. å¿…é¡»æœ‰Initial Question
    if not row.get('Initial Question', '').strip():
        return False, 'no_initial_question'
    
    # 5. å¿…é¡»æœ‰Transcript
    if not row.get('Transcript', '').strip():
        return False, 'no_transcript'
    
    # 6. æ£€æŸ¥æ˜¯å¦æ˜æ˜¾è¶…å‡ºèŒƒå›´
    initial_q = row.get('Initial Question', '').lower()
    if is_out_of_scope(initial_q):
        return False, 'out_of_scope'
    
    return True, 'ok'


def process_csv_file(csv_file: str, extraction_strategy: str = 'all') -> tuple[List[Dict], Dict]:
    """
    å¤„ç†å•ä¸ªCSVæ–‡ä»¶
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
        extraction_strategy: 'first' æˆ– 'all'
    
    Returns:
        (qa_pairsåˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    qa_pairs = []
    stats = {
        'total_chats': 0,
        'filtered_out': defaultdict(int),
        'qa_pairs_extracted': 0
    }
    
    print(f"\nğŸ“ Processing: {csv_file}")
    
    try:
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            
            for row in reader:
                stats['total_chats'] += 1
                
                # åˆ¤æ–­æ˜¯å¦åŒ…å«
                should_include, reason = should_include_chat(row)
                if not should_include:
                    stats['filtered_out'][reason] += 1
                    continue
                
                # è§£æå¯¹è¯ï¼ˆå¸¦éšç§ä¿æŠ¤ï¼‰
                messages = parse_transcript(row.get('Transcript', ''), anonymize=True)
                if not messages:
                    stats['filtered_out']['empty_messages'] += 1
                    continue
                
                # æå–å…ƒæ•°æ®ï¼ˆéšç§ä¿æŠ¤ï¼šæ›¿æ¢Answererå§“åï¼‰
                answerer = row.get('Answerer', '')
                if answerer and answerer.strip():
                    answerer = 'Librarian'  # éšç§ä¿æŠ¤
                
                metadata = {
                    'chat_id': row.get('Chat ID', ''),
                    'timestamp': row.get('Timestamp', ''),
                    'rating': int(float(row.get('Rating (0-4)', 0))) if row.get('Rating (0-4)', '').strip() else 0,
                    'duration': int(float(row.get('Duration (seconds)', 0))) if row.get('Duration (seconds)', '').strip() else 0,
                    'message_count': int(row.get('Message Count', 0)),
                    'answerer': answerer,  # å·²åŒ¿ååŒ–
                    'department': row.get('Department', ''),
                    'tags': [tag.strip() for tag in row.get('Tags', '').split(',') if tag.strip()]
                }
                
                # æ ¹æ®ç­–ç•¥æå–Q&Aå¯¹
                if extraction_strategy == 'first':
                    qa = extract_first_qa(messages, row.get('Initial Question', ''))
                    extracted_pairs = [qa] if qa else []
                else:  # 'all'
                    extracted_pairs = extract_all_qa_pairs(messages, row.get('Initial Question', ''))
                
                # å¤„ç†æ¯ä¸ªQ&Aå¯¹
                for qa in extracted_pairs:
                    if not qa or not qa.get('question') or not qa.get('answer'):
                        continue
                    
                    # åˆ†ç±»ä¸»é¢˜
                    topic = classify_topic(qa['question'], qa['answer'])
                    
                    # è·³è¿‡è¶…å‡ºèŒƒå›´çš„é—®é¢˜
                    if topic == 'out_of_scope':
                        stats['filtered_out']['out_of_scope'] += 1
                        continue
                    
                    # æå–å…³é”®è¯
                    keywords = extract_keywords(qa['question'] + ' ' + qa['answer'])
                    
                    # è®¡ç®—ç½®ä¿¡åº¦
                    confidence_score = calculate_confidence_score(qa, metadata)
                    
                    # æ„å»ºå®Œæ•´è®°å½•
                    full_qa = {
                        # æ ¸å¿ƒå†…å®¹
                        'question': qa['question'],
                        'answer': qa['answer'],
                        
                        # åˆ†ç±»
                        'topic': topic,
                        'keywords': keywords,
                        
                        # è´¨é‡
                        'rating': metadata['rating'],
                        'confidence_score': round(confidence_score, 3),
                        
                        # å…ƒæ•°æ®
                        'source': 'transcripts',
                        'chat_id': metadata['chat_id'],
                        'timestamp': metadata['timestamp'],
                        'answerer': metadata['answerer'],
                        'department': metadata['department'],
                        'tags': metadata['tags']
                    }
                    
                    qa_pairs.append(full_qa)
                    stats['qa_pairs_extracted'] += 1
    
    except Exception as e:
        print(f"âŒ Error processing {csv_file}: {e}")
        return [], stats
    
    return qa_pairs, stats


def merge_stats(stats_list: List[Dict]) -> Dict:
    """åˆå¹¶å¤šä¸ªç»Ÿè®¡ä¿¡æ¯"""
    merged = {
        'total_chats': 0,
        'filtered_out': defaultdict(int),
        'qa_pairs_extracted': 0
    }
    
    for stats in stats_list:
        merged['total_chats'] += stats['total_chats']
        merged['qa_pairs_extracted'] += stats['qa_pairs_extracted']
        for reason, count in stats['filtered_out'].items():
            merged['filtered_out'][reason] += count
    
    return merged


def print_statistics(stats: Dict):
    """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ“Š å¤„ç†ç»Ÿè®¡")
    print("="*60)
    print(f"æ€»å¯¹è¯æ•°: {stats['total_chats']}")
    print(f"æå–çš„Q&Aå¯¹: {stats['qa_pairs_extracted']}")
    print(f"è¿‡æ»¤æ‰: {sum(stats['filtered_out'].values())}")
    print("\nè¿‡æ»¤åŸå› åˆ†å¸ƒ:")
    for reason, count in sorted(stats['filtered_out'].items(), key=lambda x: x[1], reverse=True):
        print(f"  - {reason}: {count}")
    
    if stats['total_chats'] > 0:
        retention_rate = (stats['qa_pairs_extracted'] / stats['total_chats']) * 100
        print(f"\nâœ… æ•°æ®ä¿ç•™ç‡: {retention_rate:.1f}%")


def main():
    parser = argparse.ArgumentParser(description='æ¸…ç†å†å²å¯¹è¯CSVæ–‡ä»¶å‡†å¤‡RAGæ‘„å…¥ï¼ˆå¸¦éšç§ä¿æŠ¤ï¼‰')
    parser.add_argument('csv_files', nargs='+', help='CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥å¤šä¸ªï¼‰')
    parser.add_argument('--output', '-o', default='cleaned_transcripts.json', help='è¾“å‡ºJSONæ–‡ä»¶å')
    parser.add_argument('--strategy', '-s', choices=['first', 'all'], default='all',
                        help='æå–ç­–ç•¥: first=åªæå–é¦–é—®é¦–ç­”, all=æå–æ‰€æœ‰Q&Aå¯¹ (æ¨è)')
    parser.add_argument('--min-confidence', type=float, default=0.0,
                        help='æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼ (0.0-1.0)')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹å¤„ç†å†å²å¯¹è¯æ•°æ®...")
    print(f"ğŸ”’ éšç§ä¿æŠ¤: å·²å¯ç”¨ï¼ˆæ‰€æœ‰å›¾ä¹¦é¦†å‘˜å§“åå°†æ›¿æ¢ä¸º'Librarian'ï¼‰")
    print(f"æå–ç­–ç•¥: {args.strategy}")
    print(f"æœ€ä½ç½®ä¿¡åº¦: {args.min_confidence}")
    
    # å¤„ç†æ‰€æœ‰CSVæ–‡ä»¶
    all_qa_pairs = []
    all_stats = []
    
    for csv_file in args.csv_files:
        qa_pairs, stats = process_csv_file(csv_file, args.strategy)
        all_qa_pairs.extend(qa_pairs)
        all_stats.append(stats)
    
    # åˆå¹¶ç»Ÿè®¡
    merged_stats = merge_stats(all_stats)
    
    # æŒ‰ç½®ä¿¡åº¦è¿‡æ»¤
    if args.min_confidence > 0:
        before_count = len(all_qa_pairs)
        all_qa_pairs = [qa for qa in all_qa_pairs if qa['confidence_score'] >= args.min_confidence]
        after_count = len(all_qa_pairs)
        print(f"\nğŸ” ç½®ä¿¡åº¦è¿‡æ»¤: {before_count} -> {after_count} (removed {before_count - after_count})")
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(all_qa_pairs, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
    print(f"ğŸ“¦ å…± {len(all_qa_pairs)} æ¡Q&Aå¯¹")
    
    # æ‰“å°ç»Ÿè®¡
    print_statistics(merged_stats)
    
    # ä¸»é¢˜åˆ†å¸ƒç»Ÿè®¡
    print("\n" + "="*60)
    print("ğŸ“‹ ä¸»é¢˜åˆ†å¸ƒ")
    print("="*60)
    topic_counts = defaultdict(int)
    for qa in all_qa_pairs:
        topic_counts[qa['topic']] += 1
    
    for topic, count in sorted(topic_counts.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / len(all_qa_pairs)) * 100
        print(f"  - {topic}: {count} ({percentage:.1f}%)")
    
    # è´¨é‡åˆ†å¸ƒ
    print("\n" + "="*60)
    print("â­ è´¨é‡åˆ†å¸ƒ")
    print("="*60)
    confidence_ranges = {
        'Very High (0.8-1.0)': 0,
        'High (0.7-0.8)': 0,
        'Medium (0.6-0.7)': 0,
        'Low (0.5-0.6)': 0,
        'Very Low (<0.5)': 0
    }
    
    for qa in all_qa_pairs:
        conf = qa['confidence_score']
        if conf >= 0.8:
            confidence_ranges['Very High (0.8-1.0)'] += 1
        elif conf >= 0.7:
            confidence_ranges['High (0.7-0.8)'] += 1
        elif conf >= 0.6:
            confidence_ranges['Medium (0.6-0.7)'] += 1
        elif conf >= 0.5:
            confidence_ranges['Low (0.5-0.6)'] += 1
        else:
            confidence_ranges['Very Low (<0.5)'] += 1
    
    for range_name, count in confidence_ranges.items():
        if len(all_qa_pairs) > 0:
            percentage = (count / len(all_qa_pairs)) * 100
            print(f"  - {range_name}: {count} ({percentage:.1f}%)")
    
    print("\n" + "="*60)
    print("âœ¨ å¤„ç†å®Œæˆï¼ä¸‹ä¸€æ­¥:")
    print("  1. æ£€æŸ¥è¾“å‡ºæ–‡ä»¶è´¨é‡")
    print("  2. å¯é€‰: è¿è¡Œå»é‡è„šæœ¬ (deduplicate_transcripts.py)")
    print("  3. è¿è¡Œ: python scripts/ingest_transcripts.py")
    print("="*60)


if __name__ == '__main__':
    main()
